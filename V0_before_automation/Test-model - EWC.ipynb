{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 1. Load two different tasks (e.g., classify digits 0–4 and 5–9)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.\n",
    "x_test = x_test.astype(\"float32\") / 255.\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# Task A: digits 0–4\n",
    "task_a_idx = y_train < 5\n",
    "X_a, y_a = x_train[task_a_idx], y_train[task_a_idx]\n",
    "\n",
    "# Task B: digits 5–9\n",
    "task_b_idx = y_train >= 5\n",
    "X_b, y_b = x_train[task_b_idx], y_train[task_b_idx] - 5  # relabel to 0–4\n",
    "\n",
    "# 2. Define a small CNN model\n",
    "def build_model(num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(16, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(num_classes=5)\n",
    "\n",
    "# 3. Train on Task A\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_a, y_a, epochs=5, batch_size=64, verbose=0)\n",
    "\n",
    "\n",
    "# === Prepare test splits ===\n",
    "task_a_test_idx = y_test < 5\n",
    "X_a_test, y_a_test = x_test[task_a_test_idx], y_test[task_a_test_idx]\n",
    "\n",
    "task_b_test_idx = y_test >= 5\n",
    "X_b_test, y_b_test = x_test[task_b_test_idx], y_test[task_b_test_idx] - 5  # relabel to 0–4\n",
    "\n",
    "# === Evaluate after Task A training ===\n",
    "print(\"\\n✅ Evaluation after Task A training:\")\n",
    "acc_a = model.evaluate(X_a_test, y_a_test, verbose=0)[1]\n",
    "acc_b = model.evaluate(X_b_test, y_b_test, verbose=0)[1]\n",
    "print(f\"Accuracy on Task A (0–4): {acc_a:.4f}\")\n",
    "print(f\"Accuracy on Task B (5–9): {acc_b:.4f}  ← Expected to be low (not trained yet)\")\n",
    "\n",
    "\n",
    "# 4. Save weights and compute importance (approximated via gradients)\n",
    "weights_task_a = model.get_weights()\n",
    "\n",
    "# Compute importance (Fisher-like approximation using gradients on task A data)\n",
    "importance = []\n",
    "for var in model.trainable_variables:\n",
    "    importance.append(tf.zeros_like(var))\n",
    "\n",
    "batch = tf.data.Dataset.from_tensor_slices((X_a, y_a)).batch(64)\n",
    "\n",
    "for x_batch, y_batch in batch:\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, preds)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    for i, grad in enumerate(grads):\n",
    "        if grad is not None:\n",
    "            importance[i] += tf.square(grad)\n",
    "\n",
    "# Normalize importance\n",
    "importance = [imp / len(batch) for imp in importance]\n",
    "\n",
    "# 5. Fine-tune on Task B with EWC regularization\n",
    "lambda_ewc = 1000.0  # importance regularization strength\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Custom training loop with EWC\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    for step, (x_batch, y_batch) in enumerate(tf.data.Dataset.from_tensor_slices((X_b, y_b)).batch(64)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x_batch, training=True)\n",
    "            loss = loss_fn(y_batch, preds)\n",
    "            # Add EWC regularization\n",
    "            for var, old_w, imp in zip(model.trainable_variables, weights_task_a, importance):\n",
    "                loss += (lambda_ewc / 2) * tf.reduce_sum(imp * tf.square(var - old_w))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "# === Evaluate after Task B training with EWC ===\n",
    "print(\"\\n✅ Evaluation after Task B training (EWC fine-tuning):\")\n",
    "acc_a = model.evaluate(X_a_test, y_a_test, verbose=0)[1]\n",
    "acc_b = model.evaluate(X_b_test, y_b_test, verbose=0)[1]\n",
    "print(f\"Accuracy on Task A (0–4): {acc_a:.4f}  ← Should remain high if EWC works\")\n",
    "print(f\"Accuracy on Task B (5–9): {acc_b:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e56a10096f3493df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "66b4006bfb024c17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T20:50:48.988251Z",
     "start_time": "2025-06-20T20:50:48.985185Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7a5923b45912e498",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c41d0191a8d8eee4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
