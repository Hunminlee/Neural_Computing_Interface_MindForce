{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Search good hyper-parameters in Normalization\n",
    "- It seems finding a good value helps a lot in performance"
   ],
   "id": "e0b2aeb92e6f70b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:55:16.715155Z",
     "start_time": "2025-07-15T15:55:16.709553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "from urllib3.http2.probe import acquire_and_get\n",
    "\n",
    "sys.path.append(\"../Share\")\n",
    "import config, utils, baseline, Trainer, Processing_same_with_MATLAB\n",
    "import os\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "baseline_K_val = [1] #Train ~18 session data, test with 19~\n",
    "K = baseline_K_val[0]\n",
    "\n",
    "feature_names = ['Zero Crossing (ZC)', 'Slope Sign Changes (SSC)', 'Waveform Length (WL)', 'WAMP', 'Mean Absolute Value (MAV)', 'Mean Square (MS)', 'Root Mean Square (RMS)',\n",
    "                 'v-order 3 (V3)', 'log detector (LD)', 'difference absolute standard deviation value (DASDV)', 'maximum fractal length (MFL)', 'myopulse percentage rate (MPR)',\n",
    "                 'mean absolute value slope (MAVS)', 'weighted mean absolute (WMS)',\n",
    "                 'Cepstrum Coefficient 1', 'Cepstrum Coefficient 2', 'Cepstrum Coefficient 3', 'Cepstrum Coefficient Average', 'DWTC1', 'DWTC2',\n",
    "                 'DWTPC1', 'DWTPC2', 'DWTPC3']\n",
    "\n",
    "\n",
    "feature_idx = range(0,len(feature_names))\n",
    "\n",
    "fs = round(10e6 / 2048)  # 4883 Hz\n",
    "lower_cutoff = 100\n",
    "upper_cutoff = 600\n",
    "filter_b, filter_a = Processing_same_with_MATLAB.cheby2(4, 30, [lower_cutoff / (fs/2), upper_cutoff / (fs/2)], btype='bandpass')\n",
    "\n",
    "feat_mean_1ch = np.array([0.1, 0.1, 2.5, 0.0, 11.0, 229.0, 13.8, -11.0, 9.0, 3.0, 1.5, 0.0, 0.0, 2.8])\n",
    "feat_std_1ch = np.array([0.02, 0.05, 0.65, 0.02, 4.43, 303.9, 6.85, 12.18, 2.87, 0.87, 0.21, 0.04, 6.68, 1.12])\n",
    "#feat_mean = np.tile(feat_mean_1ch, (4, 1))\n",
    "#feat_std = np.tile(feat_std_1ch, (4, 1))\n",
    "\n",
    "feat_mean_1ch = np.array([0.1, 0.1,\n",
    "                          [2, 2.5, 3],\n",
    "                          0.0, 11.0, 229.0, 13.8, -11.0, 9.0, 3.0, 1.5, 0.0, 0.0, 2.8])\n",
    "feat_std_1ch = np.array([0.02, 0.05,\n",
    "                         [0,5, 0.65, 0.8],\n",
    "                         0.02, 4.43, 303.9, 6.85, 12.18, 2.87, 0.87, 0.21, 0.04, 6.68, 1.12])\n",
    "\n",
    "SUBJECT = \"Carlson\"\n",
    "data_files = config.dataset_sub_C\n",
    "default_path = config.default_path_sub_C\n",
    "\n",
    "trainer = Trainer.TremorModelTrainer(config, subject=SUBJECT)"
   ],
   "id": "79b346ce8599312",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:56:24.342463Z",
     "start_time": "2025-07-15T15:56:24.334452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "importlib.reload(Processing_same_with_MATLAB)\n",
    "\n",
    "#1D - one feature at a time\n",
    "def Train_and_test(Normalization_TF, feat_mean_1ch, feat_std_1ch, num_feature_set, target_feat_idx):\n",
    "\n",
    "    extractor = Processing_same_with_MATLAB.EMGFeatureExtractor(feat_mean_1ch, feat_std_1ch, filter_b, filter_a, Norm_bool=Normalization_TF, num_feature_set=num_feature_set)\n",
    "    X_train_all, y_train_all, X_test_all, y_test_all, X_val_all, y_val_all = [], [], [], [], [], []\n",
    "    unseen_test_result = []\n",
    "\n",
    "    for idx, session_info in enumerate(data_files):\n",
    "        print(f\"Dataset {idx + 1}/{len(data_files)} - Session {session_info}\\n{'='*40}\")\n",
    "        path = os.path.join(default_path, f'{session_info}raw/')\n",
    "        features, class_labels = [], []\n",
    "        for c_idx, c in enumerate(config.classes_5):\n",
    "            raw_data = os.listdir(path+c)\n",
    "            mat = scipy.io.loadmat(path+c+raw_data[0])\n",
    "            extractor.buffer = mat['Data_ADC']\n",
    "            class_labels.append(mat['Data_Cls'].reshape(-1))\n",
    "\n",
    "            #### features_per_cls = extractor.extract_features(num_feature_set=num_feature_set)  ### 이건 2D input\n",
    "            features_per_cls = extractor.extract_one_feature_at_a_time(target_feature_idx=target_feat_idx)  ###1D input\n",
    "\n",
    "            #여기서 Normalization_TF\n",
    "            #features = (features - self.feat_mean[:, :, np.newaxis]) / self.feat_std[:, :, np.newaxis]\n",
    "            features_per_cls = extractor.Normalization(features_per_cls, target_feat_idx)\n",
    "\n",
    "            features_per_cls = np.transpose(features_per_cls, (1, 0))  # shape: (1729, 4, 14)\n",
    "            features.append(features_per_cls)\n",
    "            #print(features_per_cls.shape, mat['Data_Cls'].reshape(-1).shape)\n",
    "\n",
    "        X = np.concatenate(features, axis=0)\n",
    "        y = np.concatenate(class_labels, axis=0)\n",
    "        if X.shape[0] != y.shape[-1]:\n",
    "            print(f\"Incorrect shape between features and Class: {X.shape} and {y.shape}, {session_info}\")\n",
    "            break\n",
    "\n",
    "        if idx < K:\n",
    "            X_train, y_train, X_val, y_val = utils.split_data(X, y, ratio=0.8)\n",
    "            X_train_all.append(X_train)\n",
    "            y_train_all.append(y_train)\n",
    "            X_val_all.append(X_train)\n",
    "            y_val_all.append(y_train)\n",
    "\n",
    "        elif idx == K:\n",
    "            X_train, y_train, X_test, y_test,  = utils.split_data(X, y, ratio=0.8)\n",
    "            X_train_all.append(X_train)\n",
    "            y_train_all.append(y_train)\n",
    "            X_val_all.append(X_test)\n",
    "            y_val_all.append(y_test)\n",
    "\n",
    "            X_train_stacked = np.concatenate(X_train_all, axis=0)\n",
    "            y_train_stacked = np.concatenate(y_train_all, axis=0)\n",
    "            print(f\"\\t Training {K}: \", X_train_stacked.shape, y_train_stacked.shape)\n",
    "            acc, pre_trained_CNN = trainer.train_multiple_dataset_1D(X_train, y_train, X_test, y_test)\n",
    "            print(f\"\\t Accuracy on test dataset {idx+1}: {acc:.4f}%\")\n",
    "\n",
    "        else:\n",
    "            X_test, y_test, _, _ = utils.split_data(X, y, ratio=1)\n",
    "            X_test_all.append(X_test)\n",
    "            y_test_all.append(y_test)\n",
    "            X_test_stacked = np.concatenate(X_test_all, axis=0)\n",
    "            y_test_stacked = np.concatenate(y_test_all, axis=0)\n",
    "\n",
    "            X = np.expand_dims(X, axis=-1)\n",
    "            acc = pre_trained_CNN.evaluate(X_test, y_test, verbose=0)[1]*100\n",
    "            print(f\"\\t Accuracy on unseen dataset {idx+1}: {acc:.4f}%\")\n",
    "            unseen_test_result.append(acc)\n",
    "\n",
    "    return unseen_test_result, X_train_stacked, y_train_stacked, X_test_stacked, y_test_stacked"
   ],
   "id": "f22a236a964d3753",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:56:45.307328Z",
     "start_time": "2025-07-15T15:56:26.033959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Normalization_TF = True ############################ important!!!!\n",
    "feature_idx = 2 #WL\n",
    "test_acc, X_train_stacked, y_train_stacked, X_test_stacked, y_test_stacked = Train_and_test(Normalization_TF, feat_mean_1ch, feat_std_1ch,\n",
    "                                                                                            num_feature_set=14, target_feat_idx=feature_idx)\n",
    "print(X_train_stacked.shape, y_train_stacked.shape, X_test_stacked.shape, y_test_stacked.shape)"
   ],
   "id": "55fff3ade0776ccf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1/8 - Session Exp_2025-06-30-v1/E9AD0E7DCC2B/\n",
      "========================================\n",
      "Dataset 2/8 - Session Exp_2025-06-30-v2/E9AD0E7DCC2B/\n",
      "========================================\n",
      "\t Training 1:  (13641, 4, 1) (13641,)\n",
      "Start Training (total epochs: 50)...\n",
      "Finish Training! (Model is NOT saved)\n",
      "\n",
      "Maximum training accuracy : 85.76%\n",
      "Maximum validation accuracy : 87.32%\n",
      "Accuracy of test dataset using model V0: 87.1386%\n",
      "\t Accuracy on test dataset 2: 87.3156%\n",
      "Dataset 3/8 - Session Exp_2025-07-09-v1/E9AD0E7DCC2B/\n",
      "========================================\n",
      "\t Accuracy on unseen dataset 3: 85.3961%\n",
      "Dataset 4/8 - Session Exp_2025-07-09-v2/E9AD0E7DCC2B/\n",
      "========================================\n",
      "\t Accuracy on unseen dataset 4: 84.0135%\n",
      "Dataset 5/8 - Session Exp_2025-07-10-v1/E9AD0E7DCC2B/\n",
      "========================================\n",
      "\t Accuracy on unseen dataset 5: 81.9119%\n",
      "Dataset 6/8 - Session Exp_2025-07-10-v2/E9AD0E7DCC2B/\n",
      "========================================\n",
      "\t Accuracy on unseen dataset 6: 83.2542%\n",
      "Dataset 7/8 - Session Exp_2025-07-11-v1/E9AD0E7DCC2B/\n",
      "========================================\n",
      "\t Accuracy on unseen dataset 7: 84.8176%\n",
      "Dataset 8/8 - Session Exp_2025-07-11-v2/E9AD0E7DCC2B/\n",
      "========================================\n",
      "\t Accuracy on unseen dataset 8: 84.6091%\n",
      "(13641, 4, 1) (13641,) (51743, 4, 1) (51743,)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:54:00.403752Z",
     "start_time": "2025-07-15T15:54:00.400139Z"
    }
   },
   "cell_type": "code",
   "source": "test_acc #Without Norm",
   "id": "44efb050ecfad66d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83.41259956359863,\n",
       " 84.05981659889221,\n",
       " 78.71378660202026,\n",
       " 81.50550127029419,\n",
       " 82.3972225189209,\n",
       " 82.63145685195923]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:56:48.547013Z",
     "start_time": "2025-07-15T15:56:48.542808Z"
    }
   },
   "cell_type": "code",
   "source": "test_acc #With Norm",
   "id": "87639b25c13a0120",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[85.39612293243408,\n",
       " 84.01345014572144,\n",
       " 81.91193342208862,\n",
       " 83.25420022010803,\n",
       " 84.81760025024414,\n",
       " 84.60912108421326]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "이제 좋은 param 찾기",
   "id": "55f3222838f27912"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c6391c92169f13fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "근데 다 같이 했을때 Normalization이 중요한거고, 다 같이 안하면 크게 상관없는 거 아닌가요\n",
    "- 아니지, 일단 좋은 param을 찾고 그거를 합치면 더 좋아질듯"
   ],
   "id": "a443e43cb24e7517",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4f394c008fee79d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "12146d3402069b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b453d6576afcf7ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e855e007febf9de3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2ecd646dd43d5a56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "51a083259181f2dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2477b045e1e43c49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "199d3223804151be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bb40247da06ef485",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "795c4aa46b38952a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5e4e40ac1a13e0e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1de730d0f00bdda0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1acacf1ff7209f6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2dce166dd5bd685d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# (13641, 4, 18, 1) (13641,) (51743, 4, 18, 1) (51743,) => X_train, y_train, X_test, y_test shape\n",
    "\n",
    "def feature_wise_norm_search(X_train_stacked, y_train_stacked, X_test_stacked, y_test_stacked,\n",
    "                            feature_idx,  # list of feature indices to test (e.g., range(14))\n",
    "                            mean_candidates,  # list of means to try (e.g., [0.0, 0.1, 0.5, 1.0])\n",
    "                            std_candidates,   # list of stds to try (e.g., [0.1, 0.5, 1.0, 2.0]),\n",
    "                            trainer\n",
    "                        ):\n",
    "    best_mean_std_list = []\n",
    "    feature_acc = []\n",
    "\n",
    "    for f_idx in feature_idx:\n",
    "        print(f\"🔍 Searching normalization params for feature {f_idx}...\")\n",
    "        best_acc = -1\n",
    "        best_mean = None\n",
    "        best_std = None\n",
    "\n",
    "        # Extract the single feature → shape: (N, 4, 1)\n",
    "        X_train_f = X_train_stacked[:, :, f_idx:f_idx+1, :]\n",
    "        X_test_f = X_test_stacked[:, :, f_idx:f_idx+1, :]\n",
    "\n",
    "        X_train_f = np.squeeze(X_train_f, axis=-1)  # (N, 4, 1)\n",
    "        X_test_f = np.squeeze(X_test_f, axis=-1)\n",
    "        print(X_train_f.shape, X_test_f.shape)\n",
    "\n",
    "        for mean in mean_candidates:\n",
    "            for std in std_candidates:\n",
    "                if std < 1e-6:  # Avoid division by zero\n",
    "                    continue\n",
    "\n",
    "                # Normalize manually\n",
    "                X_train_norm = (X_train_f - mean) / std\n",
    "                X_test_norm = (X_test_f - mean) / std\n",
    "\n",
    "                # Train and evaluate\n",
    "                acc, _ = trainer.train_multiple_dataset_1D(\n",
    "                    X_train_norm, y_train_stacked,\n",
    "                    X_test_norm, y_test_stacked\n",
    "                )\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_mean = mean\n",
    "                    best_std = std\n",
    "\n",
    "        print(f\"✅ Best for feature {f_idx}: mean={best_mean}, std={best_std}, acc={best_acc:.4f}\")\n",
    "        best_mean_std_list.append((best_mean, best_std))\n",
    "        feature_acc.append(best_acc)\n",
    "\n",
    "    return best_mean_std_list, feature_acc\n",
    "\n"
   ],
   "id": "940e84b647e687e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "feat_mean_1ch = np.array([0.1, 0.1, 2.5, 0.0, 11.0, 229.0, 13.8, -11.0, 9.0, 3.0, 1.5, 0.0, 0.0, 2.8])\n",
    "feat_std_1ch = np.array([0.02, 0.05, 0.65, 0.02, 4.43, 303.9, 6.85, 12.18, 2.87, 0.87, 0.21, 0.04, 6.68, 1.12])\n",
    "feat_mean = np.tile(feat_mean_1ch, (4, 1))\n",
    "feat_std = np.tile(feat_std_1ch, (4, 1))\n",
    "\n",
    "\n",
    "feature_idx = [2] #WL (좋았음)\n",
    "mean_candidates_lst = [2.5]#[2, 2.5, 3]\n",
    "std_candidates_lst = [0.65]#[0.5, 0.65, 0.8]\n",
    "\n",
    "print(feature_names[feature_idx[0]])\n",
    "best_mean_std_list, feature_acc = feature_wise_norm_search(X_train_stacked, y_train_stacked, X_test_stacked, y_test_stacked,\n",
    "                                                           feature_idx, mean_candidates_lst, std_candidates_lst, trainer)\n"
   ],
   "id": "19fbd092134fb6a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "feature_vis(feature_names, feature_acc_Norm)",
   "id": "73ab9b950a1dd697",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(Norm_acc, label=\"Without Normalization\", marker='o')\n",
    "plt.plot(noNorm_acc, label=\"With Normalization\", marker='s')\n",
    "plt.title(\"Feature Accuracy Comparison (Normalized vs. Not Normalized)\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c7e303b82618459d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(feature_acc_noNorm, label=\"Without Normalization\", marker='o')\n",
    "plt.plot(feature_acc_Norm, label=\"With Normalization\", marker='s')\n",
    "plt.title(\"Feature Accuracy Comparison (Normalized vs. Not Normalized)\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "4c8088cd07dc70c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "- 피쳐 하나씩 돌릴때는 Normalization 안해도 큰 손실은 없음\n",
    "- 여러개 돌릴때는 Normalization 중요\n",
    "- 비슷하다면 Normalization 을 하고 돌리고 거기서 좋은 값 찾기"
   ],
   "id": "ffb79759733dc3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e3a66d4da7d71b2f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
