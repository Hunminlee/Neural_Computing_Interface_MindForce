{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import utils\n",
    "import Model\n",
    "import config\n",
    "\n",
    "\n",
    "class ContinualLearningTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.default_path = config.default_path\n",
    "        self.classes = config.classes\n",
    "        self.dataset = config.dataset\n",
    "        self.info = config.Info\n",
    "\n",
    "        self.train_ratio = 0.5\n",
    "        self.set_epoch = 20\n",
    "        self.set_batch_size = 256\n",
    "        self.model_name = \"Cont_L_Model\"\n",
    "\n",
    "        self.init_acc_all = []\n",
    "        self.prev_acc_all = []\n",
    "        self.trained_acc_all = []\n",
    "        self.X_test_prev_all = []\n",
    "        self.y_test_prev_all = []\n",
    "\n",
    "    def init_stage(self, X_train, y_train, model):\n",
    "        weights_task = [tf.identity(var) for var in model.trainable_variables]\n",
    "        importance = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "        batch = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(self.set_batch_size)\n",
    "\n",
    "        for x_batch, y_batch in batch:\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = model(x_batch)\n",
    "                loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, preds)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            for i, grad in enumerate(grads):\n",
    "                if grad is not None:\n",
    "                    importance[i] += tf.square(grad)\n",
    "\n",
    "        importance = [imp / len(batch) for imp in importance]\n",
    "        return weights_task, importance\n",
    "\n",
    "    def adaptation_stage(self, X_train, y_train, X_test, y_test, model, weights_prev_task, importance):\n",
    "        lambda_ewc = 1000.0\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(self.set_batch_size)\n",
    "\n",
    "        acc_init = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "\n",
    "        for epoch in range(self.set_epoch):\n",
    "            for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    preds = model(x_batch, training=True)\n",
    "                    loss = loss_fn(y_batch, preds)\n",
    "                    for var, old_w, imp in zip(model.trainable_variables, weights_prev_task, importance):\n",
    "                        loss += (lambda_ewc / 2) * tf.reduce_sum(imp * tf.square(var - old_w))\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        acc_prev_data = model.evaluate(self.X_test_prev_all, self.y_test_prev_all, verbose=0)[1]\n",
    "        acc_current_stage = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "\n",
    "        return acc_init, acc_prev_data, acc_current_stage\n",
    "\n",
    "    def run(self):\n",
    "        for idx, session in enumerate(self.dataset):\n",
    "            print(f\"{'='*43}\\nDataset {idx+1}/{len(self.dataset)} - Session {session}\\n{'='*43}\")\n",
    "            path = os.path.join(self.default_path, session, 'raw/')\n",
    "\n",
    "            feature_set, labels = utils.get_dataset(path, self.classes, show_labels=False)\n",
    "            X_train, y_train, X_test, y_test = utils.split_data(feature_set, labels, ratio=self.train_ratio)\n",
    "\n",
    "            if idx == 0:\n",
    "                model = Model.Original_model_V1(X_train.shape[1:])\n",
    "                history, model = Model.Train_model(\n",
    "                    model, X_train, y_train, X_test, y_test,\n",
    "                    self.set_epoch, self.set_batch_size, self.model_name,\n",
    "                    set_verbose=0, save_model_set=True\n",
    "                )\n",
    "                acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "                self.init_acc_all, self.prev_acc_all, self.trained_acc_all = [0.2], [acc], [acc]\n",
    "                self.X_test_prev_all, self.y_test_prev_all = X_test, y_test\n",
    "            else:\n",
    "                model = load_model(f'{self.model_name}.keras')\n",
    "                weights_task, importance = self.init_stage(X_train, y_train, model)\n",
    "                init, prev, current = self.adaptation_stage(\n",
    "                    X_train, y_train, X_test, y_test, model,\n",
    "                    weights_task, importance\n",
    "                )\n",
    "                self.init_acc_all.append(init)\n",
    "                self.prev_acc_all.append(prev)\n",
    "                self.trained_acc_all.append(current)\n",
    "                self.X_test_prev_all = np.concatenate((self.X_test_prev_all, X_test), axis=0)\n",
    "                self.y_test_prev_all = np.concatenate((self.y_test_prev_all, y_test), axis=0)\n",
    "                del model\n",
    "\n",
    "        return self.init_acc_all, self.trained_acc_all, self.prev_acc_all\n",
    "\n",
    "    def plot_results(self, baselines, baseline_K):\n",
    "\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.title('Training from scratch every time', fontsize=15)\n",
    "        plt.plot(self.info, self.init_acc_all, marker='o', label='Untrained model on current data', linestyle='--')\n",
    "        plt.plot(self.info, self.prev_acc_all, marker='o', label='Model trained + tested on accumulated data')\n",
    "        plt.plot(self.info, self.trained_acc_all, marker='o', label='Model trained + tested on current split')\n",
    "\n",
    "        for idx, base in enumerate(baselines):\n",
    "            baseline_result = pd.read_csv(base)\n",
    "            plt.plot(self.info, baseline_result['Accuracy'] / 100, marker='^', label=f'Baseline V{idx} - K:{baseline_K[idx]}', linestyle='--')\n",
    "\n",
    "        plt.ylim([0, 1])\n",
    "        plt.xlabel('Date (Sessions)')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ],
   "id": "2d29af81c7885e13",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-27T19:01:32.880380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    baseline1_csv_path = 'C:/Users/hml76/PycharmProjects/Tremor_project_local/NCI_mindforce/github/Results/Baseline_results_train_with_1data.csv'\n",
    "    baseline2_csv_path = 'C:/Users/hml76/PycharmProjects/Tremor_project_local/NCI_mindforce/github/Results/Baseline_results_train_with_10data.csv'\n",
    "    baseline3_csv_path = 'C:/Users/hml76/PycharmProjects/Tremor_project_local/NCI_mindforce/github/Results/Baseline_results_train_with_14data.csv'\n",
    "    baseline4_csv_path = 'C:/Users/hml76/PycharmProjects/Tremor_project_local/NCI_mindforce/github/Results/Baseline_results_train_with_18data.csv'\n",
    "    baselines = [baseline1_csv_path, baseline2_csv_path,  baseline3_csv_path,  baseline4_csv_path]\n",
    "    baseline_K = ['1', '10', '14', '18']\n",
    "\n",
    "    cl_trainer = ContinualLearningTrainer(config)\n",
    "    Init_acc_all, Prev_acc_all, Trained_acc_all = cl_trainer.run()\n",
    "    cl_trainer.plot_results(baselines, baseline_K)\n",
    "\n",
    "    for idx, x in enumerate([Init_acc_all, Prev_acc_all, Trained_acc_all]):\n",
    "        if idx==0:\n",
    "            print(f\"Average of acc without training this data (init; unseen): {np.mean(x)*100:.2f}%\")\n",
    "        elif idx==1:\n",
    "            print(f\"Average of acc using previous test data after training: {np.mean(x)*100:.2f}%\")\n",
    "        elif idx==2:\n",
    "            print(f\"Average of acc using current test data after training: {np.mean(x)*100:.2f}%\")"
   ],
   "id": "a9fc703888190b45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Dataset 1/26 - Session Exp_2025-05-27/E8331D05289A/\n",
      "===========================================\n",
      "Start Training (total epochs: 20)...\n",
      "Finish Training! (Model is saved)\n",
      "Maximum training accuracy : 80.24%\n",
      "Maximum validation accuracy : 92.84%\n",
      "===========================================\n",
      "Dataset 2/26 - Session Exp_2025-06-18/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 3/26 - Session Exp_2025-06-20-v1/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 4/26 - Session Exp_2025-06-20-v2/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 5/26 - Session Exp_2025-06-20-v3/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 6/26 - Session Exp_2025-06-20-v4/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 7/26 - Session Exp_2025-06-20-v5/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 8/26 - Session Exp_2025-06-20-v6/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 9/26 - Session Exp_2025-06-20-v7/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 10/26 - Session Exp_2025-06-20-v8/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 11/26 - Session Exp_2025-06-23-v1/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 12/26 - Session Exp_2025-06-23-v2/E9AD0E7DCC2B/\n",
      "===========================================\n",
      "===========================================\n",
      "Dataset 13/26 - Session Exp_2025-06-23-v3/E9AD0E7DCC2B/\n",
      "===========================================\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "46df8553f8084480",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
